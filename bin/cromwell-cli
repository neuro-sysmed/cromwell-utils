#!/usr/bin/env python3

import os
import re
import sys
import argparse
from datetime import datetime, timedelta
import time
import json

sys.path.append('/cluster/lib/python3.6/site-packages/')
sys.path.append('/cluster/lib64/python3.6/site-packages/')

import pytz
import tabulate 

import kbr.args_utils as args_utils
import kbr.version_utils as version_utils
import kbr.string_utils as string_utils
import kbr.datetime_utils as datetime_utils
import kbr.file_utils as file_utils
import kbr.args_utils as args_utils



sys.path.append('.')

import cromwell.api as cromwell_api
import cromwell.facade as cromwell_facade
import cromwell.utils as cromwell_utils


version = version_utils.as_string('cromwell-utils')
as_json = False
nsm_root = '/usr/local/lib/nsm-analysis'
#nsm_root = '/home/brugger/projects/nsm/nsm-analysis'
nsm_zip  = f"{nsm_root}/nsm-analysis.zip"



def group_args(args) -> dict:

    res = {'':[]}
    for arg in args:
        m = re.match(r'(\w):(.+)', arg)
        if m is not None:
            k, v = m.group(1), m.group(2)
            if k not in res:
                res[ k ] = []
            res[ k ].append(v)
        else:
            res[ 'rest' ].append(v)
            
    return res

def cromwell_info() -> None:
    c_version = cromwell_api.get_version()
    c_status = cromwell_api.get_status()
    print(f'Cromwell server (v:{c_version}), status: {c_status}')


def workflow_subcmd(args) -> None:
    commands = {'s': 'submit', 'b': 'batch', 'st': 'status', 'a':'abort', 'r':'resubmit', 'l': 'logs',
                    'o':'outputs',  'm':'meta', 'lg': 'labels-get', 'ls': 'labels-set', 'e': 'export', 
                    'f':'fails', 'O': 'Overview', 'h':'help'} #'t': 'timing',


    args_utils.min_count(1, len(args),
                         msg="workflow takes one of the following commands: {}".format(args_utils.pretty_commands(commands)))

    command = args.pop(0)
    command = args_utils.valid_command(command, commands)

    if command == 'submit':
        cromwell_facade.submit_workflow(args, as_json=as_json)
    elif command == 'batch':
        cromwell_facade.batch_submit_workflow(args, as_json=as_json)        
    elif command == 'status':
        cromwell_facade.workflow_status(args, as_json=as_json)
    elif command == 'abort':
        cromwell_facade.workflow_abort(args, as_json=as_json)
    elif command == 'logs':
        cromwell_facade.workflow_logs(args, as_json=as_json)
    elif command == 'outputs':
        cromwell_facade.workflow_outputs(args, as_json=as_json)
    elif command == 'export':
        cromwell_facade.export_workflow_outputs(args)
    elif command == 'resubmit':
        cromwell_facade.resubmit_workflows(args, wdl_zip=nsm_zip, as_json=as_json)
#    elif command == 'timing':
#        sys.exit(10)
#        print('not done yet, do we need this? outputs a html page')
#        cromwell_facade.workflow_timing(args, as_json=as_json)
    elif command == 'meta':
        cromwell_facade.workflow_meta(args, as_json=as_json)
    elif command == 'labels-get':
            cromwell_facade.workflow_labels_get(args, as_json=as_json)
    elif command == 'labels-set':
            wf_id = args_utils.get_or_fail(args, "workflow id is required")
            cromwell_facade.workflow_labels_set(wf_id, args, as_json=as_json)
    elif command == 'fails':
            cromwell_facade.workflow_fails(args, as_json=as_json)
    elif command == 'overview':
            cromwell_facade.workflow_overview(args, as_json=as_json)

    else:

        print("Help:")
        print("Submit and interact with workflow(s)")
        print("==========================")
        print("workflow submit [wdl-file] i:[input-jsonfile(s)] o:[options-jsonfile] d:[dependency-zipfile] l:[label-jsonfile] ")
        print("workflow batch  [wdl-file] i:[input-jsonfile] o:[options-jsonfile] d:[dependency-zipfile] l:[label-jsonfile]")
        print("workflow status [job-ids]")
        print("workflow abort [job-ids]")
        print("workflow logs [job-ids]")
        print("workflow labels get [job-ids]")
        print("workflow labels set [job-ids] [labels]")
        print("workflow outputs [job-ids]")
        print("workflow meta [job-ids]")
        print("workflow fails [job-ids]")
        print("workflow overview [job-ids]")
        sys.exit(1)


def print_workflows(data:list, as_json:bool=False, brief:bool=False) -> None:
    counts = {}
    for r in data:
        name = r.get('name', 'NA')
        status = r['status']
        if name not in counts:
            counts[name] = {}
        if status not in counts[name]:
            counts[name][status] = 0
        counts[name][status] += 1

    res = [['name', 'status', 'count']]
    for name in counts:
        for status in counts[name]:
            res.append([name, status, counts[name][status]])

    print( tabulate.tabulate(res, headers="firstrow", tablefmt='psql'))



def print_workflows_analysis(data:list, as_json:bool=False) -> None:
    counts  = {}
    fails   = {}
    running = {}
    for r in data:
        name = r.get('name', 'NA')
        status = r['status']
        if name not in counts:
            counts[name] = {}
        if status not in counts[name]:
            counts[name][status] = 0
        counts[name][status] += 1

        meta = cromwell_api.workflow_meta(r['id'])

        if 'calls' in meta:
            for call in meta['calls']:
                for shard in meta['calls'][call]:
                    if "executionStatus" in shard and shard['executionStatus'] == 'Failed':
                        if call not in fails:
                            fails[ call ] = 0
                        fails[ call ] += 1

                    if "executionStatus" in shard and shard['executionStatus'] == 'Running':
                        if call not in running:
                            running[ call ] = 0
                        running[ call ] += 1

    res = [['Workflow', 'status', 'count']]
    for name in counts:
        for status in counts[name]:
            res.append([name, status, counts[name][status]])

    print( tabulate.tabulate(res, headers="firstrow", tablefmt='psql'))

    print("\n")

    res = [['Current step', 'count']]
    for name in running:
        res.append([name, running[name]])

    print( tabulate.tabulate(res, headers="firstrow", tablefmt='psql'))


    print("\n")

    res = [['Failures', 'count']]
    for name in fails:
        res.append([name, fails[name]])

    print( tabulate.tabulate(res, headers="firstrow", tablefmt='psql'))






def workflows_subcmd(args, limit:int=-1, ids_only:bool=False) -> None:
    commands = {'a':'all', 'd': 'days', 'h':'hours','s': 'status', 'n': 'name', 'i':'id', 'l': 'label', 'da':'date', 'q':'query', 'h':'help'}

    if 'b' in args:
        args[ args.index( 'b')] = 'brief'

    brief = False
    if 'brief' in args:
        brief = True
        del args[ args.index( 'brief')]

    meta = False
    if 'meta' in args:
        meta = True
        del args[ args.index( 'meta')]

    if len(args) == 0:
        args.append('l')

    args_utils.min_count(1, len(args),
                         msg="workflows takes one of the following commands: {}".format(args_utils.pretty_commands(commands)))

    command = args.pop(0)
    command = args_utils.valid_command(command, commands)

    if command == 'all':
        data = cromwell_facade.workflows(as_json=as_json)
    elif command == 'last':
        count = int(args_utils.get_or_default(args, 10))
        data = cromwell_facade.workflows(as_json=as_json, count=count)
    elif command == 'date':
        from_date = args_utils.get_or_fail(args, "from date is required")
        to_date   = args_utils.get_or_default(args, None)

        data = cromwell_facade.workflows(from_date=from_date, to_date=to_date, as_json=as_json)
    elif command == 'days':
        days   = args_utils.get_or_default(args, 7)
        from_date = datetime_utils.to_string( datetime.now(pytz.utc) - timedelta(days=int(days)) )
        data = cromwell_facade.workflows(from_date=from_date, as_json=as_json, query=True)
    elif command == 'hours':
        hours   = args_utils.get_or_default(args, 1)
        from_date = datetime_utils.to_string( datetime.now(pytz.utc) - timedelta(hours=int(hours)) )
        data = cromwell_facade.workflows(from_date=from_date, as_json=as_json, query=True)
    elif command == 'status':
        data = cromwell_facade.workflows(status=args, as_json=as_json)
    elif command == 'name':
        data = cromwell_facade.workflows(names=args, as_json=as_json)
    elif command == 'id':
        data = cromwell_facade.workflows(ids=args, as_json=as_json)
    elif command == 'label':
        data = cromwell_facade.workflows(labels=args, as_json=as_json)

    elif command == 'query':
        args = group_args(args)

        data = cromwell_facade.workflows( from_date=args.get("f", None),
                            to_date=args.get("t", None),
                            status=args.get("s", None), 
                            names=args.get("n", None), 
                            ids=args.get("i", None), 
                            labels=args.get("l", None), query=True, as_json=as_json)
    else:
        
        print("Help:")
        print("Fetches information/status for workflow(s), can be filtered in various ways")
        print("==========================")
        print("workflows [all])")
        print("monitor last <count, default 10>")
        print("workflows days [nr of days, default=7]")
        print("workflows hours [hours from now]")
        print("workflows status [status1, status2, ...]  ")
        print("workflows name [name1, name2, ...]")
        print("workflows id [id1, id2, ...]")
        print("workflows date [from-date] <end-date>  ")
        print("workflows query f:[from-date] t:[to-date] s:[status] n:[name] i:[ids] l:[labels]")

        sys.exit(1)

    if as_json:
        print(json.dumps(data))
    elif brief:
        print_workflows(data, as_json, brief)
    elif meta:
        print_workflows_analysis(data, as_json)
    else:
        res = [["id", "name", "status", "submitted", "started", "ended", "runtime"]]
        for r in data:
            if 'parentWorkflowId' in r:
                continue
            
            runtime = 'NA'
            if 'start' in r:
                end_time = datetime_utils.now(pytz.timezone('UTC'))
                if 'end' in r:
                    end_time = datetime_utils.to_datetime(r['end'])

                start_time = datetime_utils.to_datetime(r['start'])
                runtime = str(end_time - start_time)

            res.append([ r['id'], r.get('name','NA'), r['status'], r['submission'], r.get('start', 'NA'), r.get('end', 'NA'), runtime])

        res = sorted(res, key=lambda x: x[3], reverse=True)

        if int(limit) > 0:
            res = res[0:int(limit)+1]

        if ids_only:
            for r in res[1:]:
                print(r[0])
        else:
            print( tabulate.tabulate(res, headers="firstrow", tablefmt='psql'))



def cleanup_subcmd(args) -> None:


    commands = {'t': 'tmpfiles', 'f': 'files', 'n':'nuke', 'h':'help'}
    args_utils.min_count(1, len(args),
                         msg="cleanup takes one of the following commands: {}".format(args_utils.pretty_commands(commands)))

    command = args.pop(0)
    command = args_utils.valid_command(command, commands)

    if command == 'tmpfiles' or command == 'files' or command == 'nuke':
        value = args_utils.get_or_fail(args, "cleanup requires either days or hours followed by a number, or an id")
        if cromwell_utils.is_id(value):
            cromwell_facade.cleanup(action=command, ids=[value] + args)
        else:
            value = args_utils.valid_command(value, {'d':'days', 'h':'hours'})
            time_span = int(args_utils.get_or_default(args, 2))
            cromwell_facade.cleanup(action=command, time_type=value, time_span=time_span)

    else:
        
        print("Help:")
        print("Delete files created during workflow analysis")
        print("==========================")
        print("cleanup tmpfiles [id(s)] (various tmp files)")
        print("cleanup tmpfiles hours [older than hours from now, default=2]")
        print("cleanup tmpfiles days [older than days from now, default=2]")
        print("cleanup files [id(s)] (older than tmp and results files)")
        print("cleanup files hours [older than hours from now, default=2]")
        print("cleanup files days [older than days from now, default=2]")
        print("cleanup nuke [id(s)] (everything from the analysis)")
        print("cleanup nuke hours [older than hours from now, default=2]")
        print("cleanup nuke days [older than days from now, default=2]")
        sys.exit(1)



def utils_subcmd(args) -> None:
#    sub_commands = {'s':'set-paths', 'p':'patch-workflows','pv':'patch-versionfile', 'pi': 'ping', 'z':'zip', 'h':'help'}
    commands = {'p':'patch-versionfile', 'pi': 'ping', 'z':'zip', 'h':'help'}

    args_utils.min_count(1, len(args),
                         msg="utils takes one of the following commands: {}".format(args_utils.pretty_commands(commands)))

    command = args.pop(0)
    command = args_utils.valid_command(command, commands)

#    if sub_command == 'set-paths':
#        path = args_utils.get_or_fail(commands, "path to wdl-dir missing")

#        files = cromwell_utils.find_files(path, "*.wdl")
#        print( files )
#        for filename in files:
#            cromwell_utils.patch_imports(filename, files)

#        cromwell_utils.patch_version_location(path)

#    elif sub_command == 'patch-workflows':

#        files = cromwell_utils.find_files("workflows", "*.wdl") 
#        print( files )
#        for filename in files:
#            cromwell_utils.patch_workflow_imports(filename, files)

#        files = cromwell_utils.find_files("utils", "*.wdl") 
#        print( files )
#        for filename in files:
#            cromwell_utils.patch_workflow_imports(filename, files)

#        cromwell_utils.patch_version_location(".")

    if command == 'patch-versionfile':

        cromwell_utils.patch_version_location(".")


    elif command == 'ping':
        cromwell_info()

    elif command == 'zip':
        cromwell_utils.pack_dir('nsm-analysis.zip')

    else:

        print("Help:")
        print("utils that don't belong elsewhere")
        print("==========================")
        print("utils patch-versionfile: sets full path to version.json in versions.wdl")
        print("utils zip: compresses files so can be submitted")

#        print("utils set-paths [path to wdl dir]")
        print("utils ping (check server is a alive and what version is running) ")
        sys.exit(1)


def monitor_subcmd(args, interval:int=60) -> None:

    commands = {'a':'all', 'l':'last', 'd':'days', 'h':'hours','h':'status', 'n':'name', 'i':'id', 'la':'label', 'h':'help'}

    if len(args) == 0:
        args.append('all')

    tmp_args = args.copy()
    command = args.pop(0)
    command = args_utils.valid_command(command, commands)

    if command == 'help':

        print("Help:")
        print("Like workflows, but continually updates the information")
        print("==========================")
        print("monitor (all, default)")
        print("monitor last <count, default 10>")
        print("monitor days [days from now]")
        print("monitor hours [hours from now]")
        print("monitor status [status1, status2, ...]  ")
        print("monitor name [name1, name2, ...]")
        print("monitor id [id1, id2, ...]")
        print("monitor label [label1, label2, ...]")
        print("monitor query s:[status] n:[name] i:[ids] l:[labels]")
        sys.exit(1)

    global as_json
    as_json = False


    while True:
        os.system('clear')
        print(datetime.now())
        args = tmp_args.copy()
        workflows_subcmd( args )
        time.sleep(int(interval))
        continue

def main():

    commands = {'wf': 'workflow', 'wfs': 'workflows', 'm': 'monitor', 'c': 'cleanup', 'u':'utils', 'h':'help'}
    parser = argparse.ArgumentParser(description=f'cromwell-cli: command line tool for the interacting with cromwell server ({version})')

    parser.add_argument('-c', '--config', help="config file, or set env CROMWELL",
                        default=args_utils.get_env_var('CROMWELL'))
    parser.add_argument('-j', '--json-output', help="print the outputs in json format",
                        action="store_true", default=False)

    parser.add_argument('-I', '--id-only', help="Only print ID's for further processing",
                         action="store_true", default=False)

    parser.add_argument('-f', '--from-file', help="args read from file, for stdin use: '-'")
    parser.add_argument('-l', '--limit', help="top number of results to show", default=-1)
    parser.add_argument('-i', '--interval', help="update interval when monitoring", default=60)
    parser.add_argument('-v', '--verbose', default=0, action="count", help="Increase the verbosity of logging output")
    parser.add_argument('command', nargs='*', help="{}".format(args_utils.pretty_commands(commands)))   

    args = parser.parse_args()

    if args.json_output:
        global as_json
        as_json = True

    if args.from_file:
        args.command +=  cromwell_utils.read_args( args.from_file)

    args_utils.min_count(1, len(args.command),
                         msg="cromwell-cli takes one of the following commands: {}".format(args_utils.pretty_commands(commands)))

    command = args.command.pop(0)
    command = args_utils.valid_command(command, commands)

    if command == 'workflow':
        workflow_subcmd(args.command)
    elif command == 'workflows':
        workflows_subcmd(args.command, limit=args.limit, ids_only=args.id_only)
    elif command == 'monitor':
        monitor_subcmd(args.command, args.interval)
    elif command == 'cleanup':
        cleanup_subcmd(args.command)
    elif command == 'utils':
        utils_subcmd(args.command)
    else:
        print("The tool support the following commands: {}\n".format(args_utils.pretty_commands(commands)))
        parser.print_usage()
#        parser.add_argument('command', nargs='+', help="{}".format(",".join(commands)))
        sys.exit(1)

if __name__ == "__main__":
    main()
